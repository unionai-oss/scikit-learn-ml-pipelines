{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîÑ Build an ML Pipeline with scikit-learn & Union\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/unionai-oss/scikit-learn-ml-pipelines/blob/main/tutorial.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "This tutorial will walk you through building an end-to-end machine learning pipeline using scikit-learn and Union's AI workflow and inference platform. We'll download a dataset, train a machine learning model, deploy it, and track its artifacts using Union's powerful MLOps features. Although this example may seem relatively simple, all the concepts and tools used here can be applied to more complex machine learning and AI projects.\n",
    "\n",
    "By just adding a few lines of code to your Python functions, you'll be able to create a reproducible ML pipeline, taking advantage of Union's features:\n",
    "\n",
    "- Reproducible AI workflows: Ensure your ML pipeline produces the same environments every time.\n",
    "- Versioning of code and artifacts: Track changes in your code and models automatically.\n",
    "- Data Caching for faster iterations: Reuse results from previous executions to save time.\n",
    "- Declarative Infrastructure: Define your ML infrastructure needs directly in your code without worrying about provisioning.\n",
    "- Artifact Management for models and data: Automatically manage your model files and datasets.\n",
    "- Container Image Builder: Build and deploy your code in a consistent environment.\n",
    "- Local Development: Test your workflows locally before deploying them to the cloud.\n",
    "- Actors for long-running stateful containers: Handle tasks that require continuous state or interaction.\n",
    "- And more...\n",
    "\n",
    "```Python\n",
    "\n",
    "\n",
    "@task(\n",
    "    cache=True,\n",
    "    cache_version=\"4\",\n",
    "    container_image=image,\n",
    "    requests=Resources(cpu=\"2\", mem=\"2Gi\")\n",
    ")\n",
    "def download_data(): -> pd.DataFrame:\n",
    "    ...\n",
    "\n",
    "@task(\n",
    "    container_image=image,\n",
    "    requests=Resources(cpu=\"2\", mem=\"20Gi\", gpu=\"1\")\n",
    ")\n",
    "def train_model(data: pd.DataFrame:): -> pytorch.Model:\n",
    "    ...\n",
    "\n",
    "@workflow()\n",
    "def pipeline_workflow():\n",
    "    data = download_data()\n",
    "    train_model(data=data)\n",
    "    ...\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "## üß∞ Setup \n",
    "\n",
    "Sign up for a Union Serverless account at [Union.ai](https://union.ai) by clicking the \"Get Started\" button. No card required, and you'll get $30 in free credits to get started. Signing up can take a few minutes.\n",
    "\n",
    "Or you can use your [Union BYOC Enterprise](https://www.union.ai/pricing) login if you have one.\n",
    "\n",
    "### üì¶ Install Python Packages & Clone Repo\n",
    "\n",
    "Packages can be installed in your local environment using the following command using your preferred package manager from the [requirements.txt](requirements.txt) file. For example `pip install -r requirements.txt`. \n",
    "\n",
    "to clone the repo, run the following command in your environment: `git clone `\n",
    "\n",
    "If you're running this notebook in a Google Colab environment, you can install the packages and clone the GitHub repo directly in the notebook by running the following cell:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    !git clone https://github.com/unionai-oss/scikit-learn-ml-pipelines\n",
    "    %cd scikit-learn-ml-pipelines\n",
    "    !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîê Authenticate\n",
    "\n",
    "If you're using [Union BYOC Enterprise](https://www.union.ai/pricing) use: `union create login --host <union-host-url>`\n",
    "\n",
    "Otherwise, Authenticate to [Union Serverless](https://www.union.ai/) by running the command below - create an account for free at [Union.ai](https://union.ai) if you don't have one:\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!union create login --serverless --auth device-flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß© Create a Simple Workflow\n",
    "\n",
    "Before we build our ML pipeline lets build a simple workflow to understand the basics of Union's workflow system.\n",
    "\n",
    "`ImageSpec` - Allows you to specify the environment in which your task will run directly in your Python code. This includes the Python packages, CUDA version, and any additional environment setup you need. When a task is run, Union will automatically build a container image with the specified environment if it doens't already exsist and run the task in that container.\n",
    "\n",
    "`Tasks` - Tasks are the building blocks of workflows. They allow you to define a unit of work and what infrastructure to us.\n",
    "\n",
    "`Workflows` - A workflow is a collection of tasks that and defines data flow. Workflows can be run locally or in the cloud.\n",
    "\n",
    "Both tasks workflows are strongly typed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile simple_wf.py\n",
    "\n",
    "# Import libraries and modules\n",
    "# task\n",
    "from flytekit import task, workflow\n",
    "\n",
    "@task\n",
    "def hello_world(name: str) -> str:\n",
    "    return f\"Hello {name}!\"\n",
    "\n",
    "# workflow\n",
    "@workflow\n",
    "def main(name: str) -> str:\n",
    "    return hello_world(name=name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run locally\n",
    "!union run simple_wf.py main --name \"union.ai\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run on Union\n",
    "!union run --remote simple_wf.py main --name \"union remote\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!union run simple_wf.py main --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÄ ML Model Training Pipeline\n",
    "\n",
    "In this sections we'll be running tasks and workflows defined in Python under the relevant folders. \n",
    "\n",
    "Navigate to the `tasks` and `workflows` folders to see the code. if you're following along in a hosted jupyter notebook you should be able to view the code by clicking on a folder icon (usually on the left side of the screen).\n",
    "\n",
    "First we'll create a machine learning pipeline that trains a model on the iris dataset.\n",
    "\n",
    "Our workflow will have the following steps:\n",
    "- Load the iris dataset\n",
    "- Split the dataset into training and testing sets\n",
    "- Train a Random Forest model\n",
    "- Evaluate the model\n",
    "- Save model as an artifact\n",
    "- run a prediction with new data\n",
    "\n",
    "Note: Data pipelines could be seperate from model training pipelines for more complex pipelines. In this example we'll keep it simple and combine them into one workflow.\n",
    "\n",
    "navigate to the [`workflows/workflows.py`](workflows/workflows.py`workflows.py) file. Find `train_iris_classification()` function to see the code for the workflow. This workflow uses tasks defined in the [`/tasks`](tasks/data.py) folder and builds a container image from [`container.py`](containers.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!union run --remote workflows/workflows.py train_iris_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `--remote` flag is used to run the workflow in the cloud. If you want to run the workflow locally, you can remove the flag.\n",
    "\n",
    "Often times you may want to run a workflow locally to test it before running it in the cloud. This is especially useful when you're developing a new workflow or debugging an existing one.\n",
    "\n",
    "It can be useful to do some things different when running locally, like using a subset of data, save files in a different format for debugging, etc. to trigger a section of code when running locally you can use can check for `\"FLYTE_INTERNAL_EXECUTION_ID\"` variable in the code. If it's not present, the code is running locally.\n",
    "\n",
    "```python\n",
    "if \"FLYTE_INTERNAL_EXECUTION_ID\" not in os.environ:\n",
    "    # Only run this code locally\n",
    "```\n",
    "\n",
    "Take a look at your pipeline in the Union UI. You can relaunch workflows/tasks, view logs, and see the artifacts generated by the workflow.\n",
    "\n",
    "\n",
    "\n",
    "### Union Remote\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from union.remote import UnionRemote\n",
    "remote = UnionRemote()\n",
    "\n",
    "def get_latest_execution_model(limit=100):\n",
    "\n",
    "  recent_executions = remote.recent_executions(limit=limit)\n",
    "  executions = [\n",
    "      e for e in recent_executions if e.spec.launch_plan.name == \"workflows.workflows.train_iris_classification\"\n",
    "  ]\n",
    "\n",
    "  recent_ex_id = executions[0].id.name\n",
    "  execution = remote.fetch_execution(name=recent_ex_id)\n",
    "  model_uri = execution.outputs[\"o0\"].remote_source\n",
    "\n",
    "  return model_uri\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_uri = get_latest_execution_model()\n",
    "print(model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flytekit import FlyteFile  \n",
    "def make_prediction(model_uri, pred_data):\n",
    "\n",
    "  predict_task = remote.fetch_task(name=\"tasks.predict.batch_knn_predict\")\n",
    "\n",
    "\n",
    "  inputs = {\n",
    "      \"pred_data\": pred_data,\n",
    "      \"model\": FlyteFile(model_uri)\n",
    "  }\n",
    "\n",
    "  # Execute the task\n",
    "  execution = remote.execute(predict_task, inputs=inputs, wait=True)\n",
    "\n",
    "  response = execution.outputs[\"o0\"]\n",
    "\n",
    "  return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(make_prediction(model_uri, [[-3.0,-5.3,-6.3,-5.0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(make_prediction(model_uri, [[-3.0,-5.3,6999.3,7775.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Model Serving & Artifacts\n",
    "\n",
    "In this tutorial we'll show you the common ways to serve a model using Union, but you can also download or move the model to your own infrastructure.\n",
    "\n",
    "- Use a regular containers for batch inference\n",
    "- Use Actors (long running stateful) for near real-time inference\n",
    "- _Coming soon:_ Serve the model and application interface within Union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Prediction ML Workflow\n",
    "The training workflow produced a model artifact that we can use to make predictions on new data.\n",
    "\n",
    "Lets run our first prediction worflow. This workflow ... We'll see how we can use actors to run long running tasks next for faster predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !union run --remote workflows/workflows.py batch_prediction_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!union register workflows/workflows.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from union.remote import UnionRemote\n",
    "# Create a remote connection\n",
    "remote = UnionRemote()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_container(data):\n",
    "\n",
    "    inputs = {\"pred_data\": data}\n",
    "\n",
    "    workflow = remote.fetch_workflow(name=\"workflows.workflows.batch_prediction_knn\")\n",
    "    execution = remote.execute(workflow, inputs=inputs, wait=True) # wait=True will block until the execution is complete\n",
    "\n",
    "    # print(execution.outputs)\n",
    "\n",
    "    return execution.outputs['o0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict_with_container([[5.1, 3.5, 1.4, 0.2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ö° Enabling Near Real-time Predictions with Actors\n",
    "\n",
    "Union [Actors](https://docs.union.ai/serverless/user-guide/core-concepts/actors/#actors) dramatically reduce the cost of cold starts by maintaining long-running stateful environments that stay ready for use until a defined time-to-live (TTL). This persistent setup eliminates redundant initialization and unlocks several key benefits:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_actors(data):\n",
    "\n",
    "    inputs = {\"pred_data\": data}\n",
    "\n",
    "    workflow = remote.fetch_workflow(name=\"workflows.workflows.actor_prediction_knn\")\n",
    "    execution = remote.execute(workflow, inputs=inputs, wait=True) # wait=True will block until the execution is complete\n",
    "\n",
    "    # print(execution.outputs)\n",
    "\n",
    "    return execution.outputs['o0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict_with_actors([[5.1, 3.5, 1.4, 0.2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict_with_actors([[5.1, 3.5, 1.4, 0.2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict_with_actors([[5.1, 3.5, 1.4, 0.2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !union run --remote workflows/workflows.py actor_prediction_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build an application with Gradio\n",
    "\n",
    "Full app serving coming soon! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coming soon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate api key to use Union remote on external hosting.\n",
    "!union create api-key admin --name gradio-hf-app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn More About Union and Building AI Pipelines:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hope you had funand learned something new from this tutorial on building ML pipelines with Union! Creating reproducible AI workflows is a powerful way to increase productivity and collaboration accross your team. And an essential part of MLOps for deploying and managing machine learning models in production.\n",
    "\n",
    "To learn more about Union and building AI pipelines: \n",
    "- Check out the [Union Documentation](https://docs.union.ai/).\n",
    "- Contact us at [Union.ai](https://union.ai) for a demo or to learn more about Union Enterprise.\n",
    "- Join our Slack community to ask questions and share your projects with other Union users.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
